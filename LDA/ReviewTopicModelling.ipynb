{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/tictactoe/Documents/iss_plp/private_project\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/iss_plp/private_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from config import DATASET\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "## gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaMulticore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`LDA` for Topic Modelling**\n",
    "___\n",
    "\n",
    "* Assumptions:\n",
    "    * Similar topics use similar set of words\n",
    "    * Latent topics are found by searching for set of words that frequently occur together in documents across the corpus.\n",
    "* Translate thw assumptions mathematically as:\n",
    "    * Documents are probability distributions over latent topics\n",
    "    * Topics themselves are probability distributions over words\n",
    "* `LDA` represents documents as mixtures of topics that spit out words with certain probabilities.\n",
    "* It assumes that documents are produced in the following fashion:\n",
    "    * Picking a topic according to the multinomial distribution of the topics (e.g. 60% business, 20% politics, 10% food).\n",
    "    * Using the topic to generate the word itself (e.g. for food topic, the word 'apple' is generated with 60% probability, 'home' with 30% probability and so on).\n",
    "* Assuming such generative model for the corpus, `LDA` then tries to backtrack from the documents to find a set of topics and corresponding words distribution that are likely to have generated the collection.\n",
    "\n",
    "### **Backend algorithm of `LDA`**\n",
    "___\n",
    "* Given a number of topic to `K`.\n",
    "* Go through each document, and randomly assign each word in the document to one of the `K` topics.\n",
    "    * This random assignment gives you both topic representations of all the documents and word distributions of all the topics.\n",
    "* Iterate over every word in every document to improve these topics:\n",
    "    * For every word in every document, and for each topic $t$, we compute:\n",
    "        * $p(t | d)$: the proportion of words in document $d$ that are currently assigned to topic $t$.\n",
    "        * $p(w | t)$: the proportion of word $w$ over all words assigned to topic $t$ for every document.\n",
    "        * The product of the two terms, averaged over the corpus, could be taken as an estimator for $p(w | t)$.\n",
    "* In the next iteration, re-assign `w` a new topic where we choose topic `t` with probability of $p(t | w)$, which is also proportional to $p(w| t)$.\n",
    "* The process is repeated for some iterations until the topic assignments stabilizes.\n",
    "* At the end:\n",
    "    * Each document is assigned to a topic.\n",
    "    * We can search for the most probable words assigned to a topic.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load dataset**\n",
    "___\n",
    "* Load dataset\n",
    "* Convert token sequence from string sequence to list\n",
    "* Get *index2token* dictionary for all the tokens in the dataset\n",
    "* Filter out tokens that occur less than twice or more than 95% of the total tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews loaded:  823046\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overall</th>\n",
       "      <th>asin</th>\n",
       "      <th>review</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>exactly need</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>agree review opening small almost bend hook ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>love go order pack keep work include always lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>tiny opening</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Okay</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  overall        asin                                             review  \\\n",
       "0   0      5.0  7106116521                             Exactly what I needed.   \n",
       "1   1      2.0  7106116521  I agree with the other review, the opening is ...   \n",
       "2   2      4.0  7106116521  Love these... I am going to order another pack...   \n",
       "3   3      2.0  7106116521                                too tiny an opening   \n",
       "4   4      3.0  7106116521                                               Okay   \n",
       "\n",
       "                                              tokens  \n",
       "0                                       exactly need  \n",
       "1  agree review opening small almost bend hook ex...  \n",
       "2  love go order pack keep work include always lo...  \n",
       "3                                       tiny opening  \n",
       "4                                                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data path\n",
    "data_path = os.path.join(DATASET, 'AmazonFashion','TokenizedReviews.csv' )\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Total number of reviews loaded: \", len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews loaded after removal:  814211\n"
     ]
    }
   ],
   "source": [
    "## remove rows with no tokens\n",
    "df = df[df['tokens'].notna()]\n",
    "print(\"Total number of reviews loaded after removal: \", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the tokens columns to list of tokens\n",
    "df['tokens'] = df['tokens'].map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>overall</th>\n",
       "      <th>asin</th>\n",
       "      <th>review</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Exactly what I needed.</td>\n",
       "      <td>[exactly, need]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>I agree with the other review, the opening is ...</td>\n",
       "      <td>[agree, review, opening, small, almost, bend, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Love these... I am going to order another pack...</td>\n",
       "      <td>[love, go, order, pack, keep, work, include, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>too tiny an opening</td>\n",
       "      <td>[tiny, opening]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7106116521</td>\n",
       "      <td>Exactly what I wanted.</td>\n",
       "      <td>[exactly, want]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  overall        asin                                             review  \\\n",
       "0   0      5.0  7106116521                             Exactly what I needed.   \n",
       "1   1      2.0  7106116521  I agree with the other review, the opening is ...   \n",
       "2   2      4.0  7106116521  Love these... I am going to order another pack...   \n",
       "3   3      2.0  7106116521                                too tiny an opening   \n",
       "5   5      5.0  7106116521                             Exactly what I wanted.   \n",
       "\n",
       "                                              tokens  \n",
       "0                                    [exactly, need]  \n",
       "1  [agree, review, opening, small, almost, bend, ...  \n",
       "2  [love, go, order, pack, keep, work, include, a...  \n",
       "3                                    [tiny, opening]  \n",
       "5                                    [exactly, want]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> exactly\n",
      "1 --> need\n",
      "2 --> agree\n",
      "3 --> almost\n",
      "4 --> back\n",
      "5 --> be\n",
      "6 --> bend\n",
      "7 --> buy\n",
      "8 --> earring\n",
      "9 --> expensive\n"
     ]
    }
   ],
   "source": [
    "## create the index2token dictionary\n",
    "dictionary = Dictionary(df['tokens'])\n",
    "\n",
    "# print the first ten tokens in the dictionary as demonstration \n",
    "i = 0\n",
    "for idx, tk in dictionary.iteritems():\n",
    "    print(f'{idx} --> {tk}')\n",
    "    i+=1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter out tokens found in:\n",
    "# less than 2 documents \n",
    "# more than 95 percent of the documents\n",
    "dictionary.filter_extremes(no_below=2,\n",
    "                          no_above=0.95,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## for each document, we create a dictionary reporting how many words\n",
    "# and how many times those words appear\n",
    "bow_corpus = [dictionary.doc2bow(tks) for tks in df['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample token list: \n",
      "['make', 'well', 'hold', 'small', 'pack', 'ciggs', 'smoke', 'use', 'tip', 'holder', 'work', 'buy', 'different', 'last', 'year', 'fall', 'apart', 'replace', 'brand', 'zipper', 'compartment', 'hold', 'small']\n",
      "===============================\n",
      "Sample token index and count: \n",
      "7 --> buy --> Count: 1\n",
      "18 --> small --> Count: 2\n",
      "30 --> pack --> Count: 1\n",
      "34 --> work --> Count: 1\n",
      "82 --> fall --> Count: 1\n",
      "88 --> last --> Count: 1\n",
      "91 --> make --> Count: 1\n",
      "105 --> use --> Count: 1\n",
      "106 --> year --> Count: 1\n",
      "107 --> zipper --> Count: 1\n",
      "109 --> brand --> Count: 1\n",
      "111 --> hold --> Count: 2\n",
      "116 --> well --> Count: 1\n",
      "117 --> smoke --> Count: 1\n",
      "164 --> compartment --> Count: 1\n",
      "175 --> apart --> Count: 1\n",
      "176 --> different --> Count: 1\n",
      "177 --> holder --> Count: 1\n",
      "178 --> replace --> Count: 1\n",
      "179 --> tip --> Count: 1\n"
     ]
    }
   ],
   "source": [
    "## get a sample list of tokens \n",
    "sample_tks = df['tokens'].iloc[24]\n",
    "sample_bow = bow_corpus[24]\n",
    "print(\"Sample token list: \")\n",
    "print(sample_tks)\n",
    "print(\"===============================\")\n",
    "print(\"Sample token index and count: \")\n",
    "for idx, count in sample_bow:\n",
    "    print(f\"{idx} --> {dictionary[idx]} --> Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **`LDA` on the corpus bag of words**\n",
    "___\n",
    "* Documentation of `LdaMulticore` [here](https://radimrehurek.com/gensim/models/ldamulticore.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 4`\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write a function to gather all the steps above\n",
    "def get_lda_topics(bow_corpus, id2word, n_topics, n_passes, n_cores):\n",
    "    print(\"Starting LDA learning......\")\n",
    "    print(\"Learning %.d topics across %.d cores......\" %(n_topics, n_cores-1))\n",
    "    start = time.perf_counter()\n",
    "    lda_model = LdaMulticore(bow_corpus, num_topics=n_topics,\n",
    "                            id2word=id2word, passes=n_passes, \n",
    "                            workers=n_cores-1)\n",
    "    end = time.perf_counter()\n",
    "    print(\"Completed training in %.3f seconds.\" %(end-start))\n",
    "    \n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: ', idx)\n",
    "        print('Words: ', topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 4 topics across 9 cores......\n",
      "Completed training in 90.780 seconds.\n",
      "Topic:  0\n",
      "Words:  0.020*\"wear\" + 0.017*\"well\" + 0.014*\"fit\" + 0.014*\"old\" + 0.013*\"wash\" + 0.013*\"be\" + 0.013*\"year\" + 0.011*\"buy\" + 0.010*\"warm\" + 0.010*\"get\"\n",
      "Topic:  1\n",
      "Words:  0.101*\"love\" + 0.057*\"great\" + 0.038*\"fit\" + 0.026*\"perfect\" + 0.025*\"good\" + 0.024*\"look\" + 0.021*\"quality\" + 0.019*\"buy\" + 0.017*\"nice\" + 0.017*\"cute\"\n",
      "Topic:  2\n",
      "Words:  0.033*\"size\" + 0.031*\"small\" + 0.029*\"be\" + 0.026*\"fit\" + 0.022*\"order\" + 0.018*\"look\" + 0.015*\"wear\" + 0.015*\"dress\" + 0.015*\"large\" + 0.011*\"little\"\n",
      "Topic:  3\n",
      "Words:  0.014*\"be\" + 0.014*\"get\" + 0.012*\"use\" + 0.011*\"make\" + 0.010*\"wear\" + 0.009*\"bag\" + 0.009*\"time\" + 0.008*\"look\" + 0.007*\"back\" + 0.007*\"buy\"\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 4\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "get_lda_topics(bow_corpus, dictionary, N_TOPICS, N_PASSES, N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 5 topics across 9 cores......\n",
      "Completed training in 89.056 seconds.\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 5\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "print(\"Starting LDA learning......\")\n",
    "print(\"Learning %.d topics across %.d cores......\" %(N_TOPICS, N_CORES-1))\n",
    "start = time.perf_counter()\n",
    "lda_model = LdaMulticore(bow_corpus, num_topics=N_TOPICS,\n",
    "                        id2word=dictionary, passes=N_PASSES, \n",
    "                        workers=N_CORES-1)\n",
    "end = time.perf_counter()\n",
    "print(\"Completed training in %.3f seconds.\" %(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  0\n",
      "Words:  0.116*\"love\" + 0.068*\"great\" + 0.043*\"fit\" + 0.029*\"perfect\" + 0.023*\"buy\" + 0.021*\"wear\" + 0.018*\"look\" + 0.017*\"comfortable\" + 0.016*\"get\" + 0.016*\"color\"\n",
      "Topic:  1\n",
      "Words:  0.025*\"look\" + 0.023*\"be\" + 0.016*\"get\" + 0.014*\"ring\" + 0.012*\"buy\" + 0.012*\"wear\" + 0.011*\"pretty\" + 0.010*\"gift\" + 0.010*\"love\" + 0.009*\"beautiful\"\n",
      "Topic:  2\n",
      "Words:  0.017*\"wear\" + 0.014*\"use\" + 0.014*\"get\" + 0.012*\"be\" + 0.012*\"time\" + 0.010*\"work\" + 0.009*\"keep\" + 0.009*\"shoe\" + 0.008*\"make\" + 0.008*\"well\"\n",
      "Topic:  3\n",
      "Words:  0.035*\"good\" + 0.033*\"nice\" + 0.032*\"quality\" + 0.023*\"look\" + 0.018*\"product\" + 0.016*\"color\" + 0.015*\"well\" + 0.015*\"price\" + 0.014*\"make\" + 0.014*\"expect\"\n",
      "Topic:  4\n",
      "Words:  0.039*\"size\" + 0.037*\"small\" + 0.034*\"fit\" + 0.032*\"be\" + 0.022*\"order\" + 0.019*\"large\" + 0.018*\"wear\" + 0.018*\"dress\" + 0.014*\"little\" + 0.014*\"big\"\n"
     ]
    }
   ],
   "source": [
    "## view the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: ', idx)\n",
    "    print('Words: ', topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write a function to gather all the steps above\n",
    "def get_lda_topics(bow_corpus, id2word, n_topics, n_passes, n_cores):\n",
    "    print(\"Starting LDA learning......\")\n",
    "    print(\"Learning %.d topics across %.d cores......\" %(n_topics, n_cores-1))\n",
    "    start = time.perf_counter()\n",
    "    lda_model = LdaMulticore(bow_corpus, num_topics=n_topics,\n",
    "                            id2word=id2word, passes=n_passes, \n",
    "                            workers=n_cores-1)\n",
    "    end = time.perf_counter()\n",
    "    print(\"Completed training in %.3f seconds.\" %(end-start))\n",
    "    \n",
    "    for idx, topic in lda_model.print_topics(-1):\n",
    "        print('Topic: ', idx)\n",
    "        print('Words: ', topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 6 topics across 9 cores......\n",
      "Completed training in 88.923 seconds.\n",
      "Topic:  0\n",
      "Words:  0.054*\"size\" + 0.048*\"small\" + 0.033*\"fit\" + 0.031*\"order\" + 0.030*\"be\" + 0.023*\"large\" + 0.020*\"wear\" + 0.017*\"dress\" + 0.016*\"big\" + 0.014*\"get\"\n",
      "Topic:  1\n",
      "Words:  0.091*\"love\" + 0.024*\"beautiful\" + 0.021*\"wear\" + 0.021*\"look\" + 0.020*\"buy\" + 0.019*\"get\" + 0.018*\"perfect\" + 0.017*\"great\" + 0.015*\"gift\" + 0.014*\"ring\"\n",
      "Topic:  2\n",
      "Words:  0.026*\"bag\" + 0.018*\"make\" + 0.017*\"great\" + 0.017*\"well\" + 0.016*\"pocket\" + 0.015*\"product\" + 0.014*\"be\" + 0.014*\"wallet\" + 0.012*\"love\" + 0.012*\"purse\"\n",
      "Topic:  3\n",
      "Words:  0.018*\"get\" + 0.015*\"wear\" + 0.015*\"be\" + 0.015*\"use\" + 0.013*\"time\" + 0.010*\"belt\" + 0.009*\"first\" + 0.009*\"day\" + 0.009*\"break\" + 0.008*\"put\"\n",
      "Topic:  4\n",
      "Words:  0.037*\"good\" + 0.033*\"look\" + 0.025*\"cheap\" + 0.024*\"quality\" + 0.015*\"make\" + 0.013*\"picture\" + 0.012*\"buy\" + 0.012*\"material\" + 0.012*\"shoe\" + 0.011*\"price\"\n",
      "Topic:  5\n",
      "Words:  0.049*\"fit\" + 0.041*\"great\" + 0.032*\"shirt\" + 0.030*\"nice\" + 0.023*\"cute\" + 0.021*\"color\" + 0.021*\"look\" + 0.020*\"love\" + 0.018*\"be\" + 0.018*\"expect\"\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 6\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "get_lda_topics(bow_corpus, dictionary, N_TOPICS, N_PASSES, N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 7`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 7 topics across 9 cores......\n",
      "Completed training in 83.763 seconds.\n",
      "Topic:  0\n",
      "Words:  0.017*\"be\" + 0.015*\"get\" + 0.014*\"bag\" + 0.012*\"make\" + 0.011*\"back\" + 0.010*\"return\" + 0.009*\"look\" + 0.009*\"use\" + 0.008*\"pocket\" + 0.008*\"come\"\n",
      "Topic:  1\n",
      "Words:  0.047*\"great\" + 0.041*\"quality\" + 0.040*\"good\" + 0.030*\"price\" + 0.028*\"look\" + 0.024*\"beautiful\" + 0.021*\"product\" + 0.020*\"love\" + 0.017*\"ring\" + 0.013*\"nice\"\n",
      "Topic:  2\n",
      "Words:  0.024*\"shoe\" + 0.024*\"wear\" + 0.022*\"work\" + 0.020*\"use\" + 0.016*\"pair\" + 0.015*\"watch\" + 0.014*\"great\" + 0.012*\"foot\" + 0.012*\"well\" + 0.012*\"time\"\n",
      "Topic:  3\n",
      "Words:  0.054*\"fit\" + 0.053*\"size\" + 0.046*\"small\" + 0.029*\"order\" + 0.027*\"be\" + 0.027*\"dress\" + 0.023*\"large\" + 0.018*\"wear\" + 0.014*\"big\" + 0.013*\"shirt\"\n",
      "Topic:  4\n",
      "Words:  0.030*\"be\" + 0.015*\"look\" + 0.013*\"well\" + 0.012*\"little\" + 0.012*\"wear\" + 0.012*\"make\" + 0.011*\"review\" + 0.010*\"fit\" + 0.008*\"cute\" + 0.007*\"head\"\n",
      "Topic:  5\n",
      "Words:  0.131*\"love\" + 0.037*\"wear\" + 0.024*\"get\" + 0.024*\"be\" + 0.023*\"comfortable\" + 0.022*\"fit\" + 0.021*\"buy\" + 0.016*\"warm\" + 0.016*\"daughter\" + 0.014*\"great\"\n",
      "Topic:  6\n",
      "Words:  0.051*\"color\" + 0.036*\"look\" + 0.034*\"nice\" + 0.024*\"cute\" + 0.023*\"love\" + 0.023*\"shirt\" + 0.022*\"picture\" + 0.020*\"like\" + 0.017*\"material\" + 0.016*\"really\"\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 7\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "get_lda_topics(bow_corpus, dictionary, N_TOPICS, N_PASSES, N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 8 topics across 9 cores......\n",
      "Completed training in 82.589 seconds.\n",
      "Topic:  0\n",
      "Words:  0.177*\"love\" + 0.047*\"fit\" + 0.041*\"perfect\" + 0.031*\"cute\" + 0.029*\"buy\" + 0.026*\"comfortable\" + 0.023*\"wear\" + 0.022*\"daughter\" + 0.020*\"great\" + 0.018*\"soft\"\n",
      "Topic:  1\n",
      "Words:  0.021*\"bag\" + 0.014*\"use\" + 0.014*\"well\" + 0.014*\"be\" + 0.013*\"keep\" + 0.013*\"great\" + 0.012*\"strap\" + 0.012*\"work\" + 0.012*\"make\" + 0.011*\"pocket\"\n",
      "Topic:  2\n",
      "Words:  0.077*\"great\" + 0.031*\"look\" + 0.030*\"price\" + 0.030*\"quality\" + 0.029*\"beautiful\" + 0.022*\"fit\" + 0.021*\"product\" + 0.021*\"good\" + 0.020*\"love\" + 0.018*\"well\"\n",
      "Topic:  3\n",
      "Words:  0.030*\"old\" + 0.030*\"year\" + 0.018*\"wallet\" + 0.018*\"use\" + 0.018*\"be\" + 0.016*\"month\" + 0.016*\"get\" + 0.012*\"card\" + 0.011*\"purse\" + 0.010*\"buy\"\n",
      "Topic:  4\n",
      "Words:  0.017*\"get\" + 0.014*\"be\" + 0.013*\"look\" + 0.013*\"wear\" + 0.012*\"make\" + 0.011*\"time\" + 0.011*\"ring\" + 0.010*\"product\" + 0.010*\"review\" + 0.009*\"buy\"\n",
      "Topic:  5\n",
      "Words:  0.054*\"size\" + 0.050*\"small\" + 0.036*\"fit\" + 0.032*\"be\" + 0.031*\"order\" + 0.024*\"large\" + 0.021*\"dress\" + 0.017*\"wear\" + 0.015*\"big\" + 0.013*\"top\"\n",
      "Topic:  6\n",
      "Words:  0.059*\"nice\" + 0.050*\"look\" + 0.045*\"good\" + 0.044*\"color\" + 0.033*\"picture\" + 0.027*\"quality\" + 0.021*\"be\" + 0.019*\"expect\" + 0.018*\"fit\" + 0.018*\"really\"\n",
      "Topic:  7\n",
      "Words:  0.043*\"shirt\" + 0.032*\"wear\" + 0.020*\"shoe\" + 0.016*\"well\" + 0.015*\"comfortable\" + 0.015*\"fit\" + 0.015*\"wash\" + 0.013*\"great\" + 0.012*\"pair\" + 0.011*\"good\"\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 8\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "get_lda_topics(bow_corpus, dictionary, N_TOPICS, N_PASSES, N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 9`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 9 topics across 9 cores......\n",
      "Completed training in 79.466 seconds.\n",
      "Topic:  0\n",
      "Words:  0.030*\"color\" + 0.026*\"shirt\" + 0.024*\"be\" + 0.024*\"look\" + 0.023*\"wear\" + 0.020*\"material\" + 0.016*\"picture\" + 0.015*\"nice\" + 0.014*\"dress\" + 0.013*\"love\"\n",
      "Topic:  1\n",
      "Words:  0.167*\"love\" + 0.075*\"cute\" + 0.025*\"super\" + 0.024*\"little\" + 0.021*\"big\" + 0.020*\"be\" + 0.018*\"daughter\" + 0.015*\"fit\" + 0.015*\"head\" + 0.014*\"really\"\n",
      "Topic:  2\n",
      "Words:  0.052*\"wear\" + 0.043*\"great\" + 0.033*\"love\" + 0.028*\"comfortable\" + 0.027*\"shoe\" + 0.022*\"work\" + 0.021*\"fit\" + 0.020*\"get\" + 0.017*\"pair\" + 0.016*\"buy\"\n",
      "Topic:  3\n",
      "Words:  0.030*\"size\" + 0.024*\"review\" + 0.019*\"year\" + 0.016*\"receive\" + 0.016*\"product\" + 0.015*\"purchase\" + 0.015*\"watch\" + 0.014*\"old\" + 0.013*\"get\" + 0.013*\"buy\"\n",
      "Topic:  4\n",
      "Words:  0.051*\"size\" + 0.051*\"small\" + 0.041*\"fit\" + 0.033*\"be\" + 0.030*\"order\" + 0.025*\"large\" + 0.022*\"dress\" + 0.016*\"wear\" + 0.014*\"big\" + 0.013*\"top\"\n",
      "Topic:  5\n",
      "Words:  0.034*\"bag\" + 0.021*\"pocket\" + 0.019*\"use\" + 0.019*\"wallet\" + 0.017*\"be\" + 0.016*\"hold\" + 0.016*\"like\" + 0.016*\"purse\" + 0.013*\"zipper\" + 0.013*\"nice\"\n",
      "Topic:  6\n",
      "Words:  0.023*\"look\" + 0.021*\"cheap\" + 0.019*\"make\" + 0.017*\"be\" + 0.017*\"get\" + 0.010*\"money\" + 0.009*\"return\" + 0.009*\"back\" + 0.008*\"quality\" + 0.008*\"time\"\n",
      "Topic:  7\n",
      "Words:  0.084*\"great\" + 0.070*\"quality\" + 0.069*\"good\" + 0.061*\"fit\" + 0.038*\"price\" + 0.035*\"nice\" + 0.032*\"expect\" + 0.032*\"look\" + 0.026*\"product\" + 0.026*\"perfect\"\n",
      "Topic:  8\n",
      "Words:  0.025*\"beautiful\" + 0.022*\"ring\" + 0.019*\"be\" + 0.019*\"look\" + 0.016*\"wear\" + 0.016*\"love\" + 0.014*\"belt\" + 0.014*\"get\" + 0.013*\"gift\" + 0.012*\"buy\"\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 9\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "get_lda_topics(bow_corpus, dictionary, N_TOPICS, N_PASSES, N_CORES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Topics = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LDA learning......\n",
      "Learning 10 topics across 9 cores......\n",
      "Completed training in 79.497 seconds.\n",
      "Topic:  0\n",
      "Words:  0.045*\"love\" + 0.025*\"wear\" + 0.024*\"ring\" + 0.023*\"gift\" + 0.018*\"get\" + 0.018*\"be\" + 0.016*\"buy\" + 0.016*\"watch\" + 0.015*\"break\" + 0.013*\"look\"\n",
      "Topic:  1\n",
      "Words:  0.084*\"size\" + 0.080*\"small\" + 0.043*\"order\" + 0.041*\"fit\" + 0.038*\"large\" + 0.030*\"be\" + 0.028*\"big\" + 0.018*\"wear\" + 0.017*\"run\" + 0.016*\"medium\"\n",
      "Topic:  2\n",
      "Words:  0.110*\"love\" + 0.076*\"fit\" + 0.072*\"great\" + 0.050*\"perfect\" + 0.023*\"buy\" + 0.021*\"old\" + 0.019*\"comfortable\" + 0.018*\"daughter\" + 0.017*\"year\" + 0.017*\"wear\"\n",
      "Topic:  3\n",
      "Words:  0.032*\"cute\" + 0.022*\"be\" + 0.021*\"wear\" + 0.017*\"get\" + 0.011*\"super\" + 0.011*\"wash\" + 0.011*\"make\" + 0.011*\"well\" + 0.010*\"really\" + 0.010*\"back\"\n",
      "Topic:  4\n",
      "Words:  0.044*\"shoe\" + 0.031*\"pair\" + 0.031*\"wear\" + 0.023*\"foot\" + 0.021*\"sock\" + 0.015*\"good\" + 0.015*\"comfortable\" + 0.013*\"boot\" + 0.012*\"buy\" + 0.011*\"work\"\n",
      "Topic:  5\n",
      "Words:  0.022*\"review\" + 0.022*\"product\" + 0.017*\"receive\" + 0.015*\"be\" + 0.012*\"get\" + 0.011*\"make\" + 0.010*\"honest\" + 0.009*\"discount\" + 0.009*\"purchase\" + 0.009*\"say\"\n",
      "Topic:  6\n",
      "Words:  0.037*\"dress\" + 0.034*\"be\" + 0.024*\"fit\" + 0.021*\"wear\" + 0.017*\"short\" + 0.017*\"material\" + 0.016*\"look\" + 0.016*\"long\" + 0.015*\"shirt\" + 0.012*\"fabric\"\n",
      "Topic:  7\n",
      "Words:  0.039*\"bag\" + 0.023*\"pocket\" + 0.019*\"wallet\" + 0.019*\"use\" + 0.017*\"purse\" + 0.016*\"be\" + 0.014*\"zipper\" + 0.014*\"hold\" + 0.013*\"like\" + 0.013*\"card\"\n",
      "Topic:  8\n",
      "Words:  0.033*\"look\" + 0.032*\"picture\" + 0.030*\"cheap\" + 0.022*\"return\" + 0.021*\"belt\" + 0.018*\"money\" + 0.018*\"make\" + 0.017*\"item\" + 0.017*\"thank\" + 0.014*\"quality\"\n",
      "Topic:  9\n",
      "Words:  0.063*\"good\" + 0.054*\"nice\" + 0.053*\"quality\" + 0.044*\"look\" + 0.041*\"great\" + 0.036*\"price\" + 0.030*\"color\" + 0.026*\"love\" + 0.022*\"beautiful\" + 0.019*\"expect\"\n"
     ]
    }
   ],
   "source": [
    "N_TOPICS = 10\n",
    "N_PASSES = 3 # number of passes through the corpus during training\n",
    "N_CORES = 10\n",
    "\n",
    "get_lda_topics(bow_corpus, dictionary, N_TOPICS, N_PASSES, N_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
