{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\onnwe\\Desktop\\ISS_PLP_Project\\Sentiment Analysis with Aspect\n",
      "C:\\Users\\onnwe\\Desktop\\ISS_PLP_Project\\Data\n",
      "C:\\Users\\onnwe\\Desktop\\ISS_PLP_Project\\SentimentModels\\Inference\\Full_Amazon\n",
      "C:\\Users\\onnwe\\Desktop\\ISS_PLP_Project\\SentimentModels\\Inference\\Shopee_Aspect\n",
      "C:\\Users\\onnwe\\Desktop\\ISS_PLP_Project\\WordNet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "import pickle as pk\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "folderpath = Path(os.getcwd()).absolute()\n",
    "data_folderpath = folderpath.parent.joinpath(\"Data\")\n",
    "amazon_model_folderpath = folderpath.parent.joinpath(\"SentimentModels\").joinpath(\"Inference\").joinpath(\"Full_Amazon\")\n",
    "shopee_model_folderpath = folderpath.parent.joinpath(\"SentimentModels\").joinpath(\"Inference\").joinpath(\"Shopee_Aspect\")\n",
    "wordnet_folderpath = folderpath.parent.joinpath(\"WordNet\")\n",
    "\n",
    "print(folderpath)\n",
    "print(data_folderpath)\n",
    "print(amazon_model_folderpath)\n",
    "print(shopee_model_folderpath)\n",
    "print(wordnet_folderpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filename = \"meta_AMAZON_FASHION.json\"\n",
    "\n",
    "filepath = data_folderpath.joinpath(filename)\n",
    "meta = pd.read_json(filepath, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>brand</th>\n",
       "      <th>feature</th>\n",
       "      <th>rank</th>\n",
       "      <th>date</th>\n",
       "      <th>asin</th>\n",
       "      <th>imageURL</th>\n",
       "      <th>imageURLHighRes</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>also_view</th>\n",
       "      <th>also_buy</th>\n",
       "      <th>fit</th>\n",
       "      <th>details</th>\n",
       "      <th>similar_item</th>\n",
       "      <th>tech1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Slime Time Fall Fest [With CDROM and Collector...</td>\n",
       "      <td>Group Publishing (CO)</td>\n",
       "      <td>[Product Dimensions:\\n                    \\n8....</td>\n",
       "      <td>13,052,976inClothing,Shoesamp;Jewelry(</td>\n",
       "      <td>8.70 inches</td>\n",
       "      <td>0764443682</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XCC Qi promise new spider snake preparing men'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11,654,581inClothing,Shoesamp;Jewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>1291691480</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Magical Things I Really Do Do Too!</td>\n",
       "      <td>Christopher Manos</td>\n",
       "      <td>[Package Dimensions:\\n                    \\n8....</td>\n",
       "      <td>19,308,073inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>1940280001</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[For the professional or amateur magician.  Ro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ashes to Ashes, Oranges to Oranges</td>\n",
       "      <td>Flickerlamp Publishing</td>\n",
       "      <td>[Package Dimensions:\\n                    \\n8....</td>\n",
       "      <td>19,734,184inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>1940735033</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aether &amp; Empire #1 - 2016 First Printing Comic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Package Dimensions:\\n                    \\n10...</td>\n",
       "      <td>10,558,646inClothing,Shoesamp;Jewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>1940967805</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$4.50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186632</th>\n",
       "      <td>JT Women's Elegant Off Shoulder Chiffon Maxi L...</td>\n",
       "      <td>JT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9,835,890inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>B01HJGXL4O</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186633</th>\n",
       "      <td>Microcosm Retro Vintage Black Crochet Lace One...</td>\n",
       "      <td>Microcosm</td>\n",
       "      <td>[Package Dimensions:\\n                    \\n7....</td>\n",
       "      <td>11,390,771inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star5 star (0%)</td>\n",
       "      <td>B01HJHF97K</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186634</th>\n",
       "      <td>Lookatool Classic Plain Vintage Army Military ...</td>\n",
       "      <td>Lookatool</td>\n",
       "      <td>[Cotton+Polyester, Imported, Item type:Basebal...</td>\n",
       "      <td>972,275inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>B01HJGJ9LS</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$8.53</td>\n",
       "      <td>[B00XLECZMS, B0018MQAOY, B00N833I4Q, B074DQSPP...</td>\n",
       "      <td>[B07BHQ1FXL, B00XLECZMS, B07CJWM5WY, B07CS97C1...</td>\n",
       "      <td>class=\"a-normal a-align-center a-spacing-smal...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186635</th>\n",
       "      <td>Edith Windsor Women's Deep V-neck Beaded Sequi...</td>\n",
       "      <td>Edith Windsor</td>\n",
       "      <td>[Product Dimensions:\\n                    \\n9....</td>\n",
       "      <td>1,964,585inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>B01HJHTH5U</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[B077ZLGMJ3]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186636</th>\n",
       "      <td>Aeropostale Women's Sun &amp; Waves Crop Cami L Gr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[Product Dimensions:\\n                    \\n5 ...</td>\n",
       "      <td>9,379,125inClothing,ShoesJewelry(</td>\n",
       "      <td>5 star</td>\n",
       "      <td>B01HJFNU7S</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>[https://images-na.ssl-images-amazon.com/image...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186637 rows Ã— 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0       Slime Time Fall Fest [With CDROM and Collector...   \n",
       "1       XCC Qi promise new spider snake preparing men'...   \n",
       "2                      Magical Things I Really Do Do Too!   \n",
       "3                      Ashes to Ashes, Oranges to Oranges   \n",
       "4       Aether & Empire #1 - 2016 First Printing Comic...   \n",
       "...                                                   ...   \n",
       "186632  JT Women's Elegant Off Shoulder Chiffon Maxi L...   \n",
       "186633  Microcosm Retro Vintage Black Crochet Lace One...   \n",
       "186634  Lookatool Classic Plain Vintage Army Military ...   \n",
       "186635  Edith Windsor Women's Deep V-neck Beaded Sequi...   \n",
       "186636  Aeropostale Women's Sun & Waves Crop Cami L Gr...   \n",
       "\n",
       "                         brand  \\\n",
       "0        Group Publishing (CO)   \n",
       "1                          NaN   \n",
       "2            Christopher Manos   \n",
       "3       Flickerlamp Publishing   \n",
       "4                          NaN   \n",
       "...                        ...   \n",
       "186632                      JT   \n",
       "186633               Microcosm   \n",
       "186634               Lookatool   \n",
       "186635           Edith Windsor   \n",
       "186636                     NaN   \n",
       "\n",
       "                                                  feature  \\\n",
       "0       [Product Dimensions:\\n                    \\n8....   \n",
       "1                                                     NaN   \n",
       "2       [Package Dimensions:\\n                    \\n8....   \n",
       "3       [Package Dimensions:\\n                    \\n8....   \n",
       "4       [Package Dimensions:\\n                    \\n10...   \n",
       "...                                                   ...   \n",
       "186632                                                NaN   \n",
       "186633  [Package Dimensions:\\n                    \\n7....   \n",
       "186634  [Cotton+Polyester, Imported, Item type:Basebal...   \n",
       "186635  [Product Dimensions:\\n                    \\n9....   \n",
       "186636  [Product Dimensions:\\n                    \\n5 ...   \n",
       "\n",
       "                                          rank               date        asin  \\\n",
       "0       13,052,976inClothing,Shoesamp;Jewelry(        8.70 inches  0764443682   \n",
       "1       11,654,581inClothing,Shoesamp;Jewelry(             5 star  1291691480   \n",
       "2           19,308,073inClothing,ShoesJewelry(             5 star  1940280001   \n",
       "3           19,734,184inClothing,ShoesJewelry(             5 star  1940735033   \n",
       "4       10,558,646inClothing,Shoesamp;Jewelry(             5 star  1940967805   \n",
       "...                                        ...                ...         ...   \n",
       "186632       9,835,890inClothing,ShoesJewelry(             5 star  B01HJGXL4O   \n",
       "186633      11,390,771inClothing,ShoesJewelry(  5 star5 star (0%)  B01HJHF97K   \n",
       "186634         972,275inClothing,ShoesJewelry(             5 star  B01HJGJ9LS   \n",
       "186635       1,964,585inClothing,ShoesJewelry(             5 star  B01HJHTH5U   \n",
       "186636       9,379,125inClothing,ShoesJewelry(             5 star  B01HJFNU7S   \n",
       "\n",
       "                                                 imageURL  \\\n",
       "0       [https://images-na.ssl-images-amazon.com/image...   \n",
       "1       [https://images-na.ssl-images-amazon.com/image...   \n",
       "2       [https://images-na.ssl-images-amazon.com/image...   \n",
       "3       [https://images-na.ssl-images-amazon.com/image...   \n",
       "4       [https://images-na.ssl-images-amazon.com/image...   \n",
       "...                                                   ...   \n",
       "186632  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186633  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186634  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186635  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186636  [https://images-na.ssl-images-amazon.com/image...   \n",
       "\n",
       "                                          imageURLHighRes  \\\n",
       "0       [https://images-na.ssl-images-amazon.com/image...   \n",
       "1       [https://images-na.ssl-images-amazon.com/image...   \n",
       "2       [https://images-na.ssl-images-amazon.com/image...   \n",
       "3       [https://images-na.ssl-images-amazon.com/image...   \n",
       "4       [https://images-na.ssl-images-amazon.com/image...   \n",
       "...                                                   ...   \n",
       "186632  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186633  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186634  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186635  [https://images-na.ssl-images-amazon.com/image...   \n",
       "186636  [https://images-na.ssl-images-amazon.com/image...   \n",
       "\n",
       "                                              description  price  \\\n",
       "0                                                     NaN    NaN   \n",
       "1                                                     NaN    NaN   \n",
       "2       [For the professional or amateur magician.  Ro...    NaN   \n",
       "3                                                     NaN    NaN   \n",
       "4                                                     NaN  $4.50   \n",
       "...                                                   ...    ...   \n",
       "186632                                                NaN    NaN   \n",
       "186633                                                NaN    NaN   \n",
       "186634                                                NaN  $8.53   \n",
       "186635                                                NaN    NaN   \n",
       "186636                                                NaN    NaN   \n",
       "\n",
       "                                                also_view  \\\n",
       "0                                                     NaN   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3                                                     NaN   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "186632                                                NaN   \n",
       "186633                                                NaN   \n",
       "186634  [B00XLECZMS, B0018MQAOY, B00N833I4Q, B074DQSPP...   \n",
       "186635                                                NaN   \n",
       "186636                                                NaN   \n",
       "\n",
       "                                                 also_buy  \\\n",
       "0                                                     NaN   \n",
       "1                                                     NaN   \n",
       "2                                                     NaN   \n",
       "3                                                     NaN   \n",
       "4                                                     NaN   \n",
       "...                                                   ...   \n",
       "186632                                                NaN   \n",
       "186633                                                NaN   \n",
       "186634  [B07BHQ1FXL, B00XLECZMS, B07CJWM5WY, B07CS97C1...   \n",
       "186635                                       [B077ZLGMJ3]   \n",
       "186636                                                NaN   \n",
       "\n",
       "                                                      fit details  \\\n",
       "0                                                     NaN     NaN   \n",
       "1                                                     NaN     NaN   \n",
       "2                                                     NaN     NaN   \n",
       "3                                                     NaN     NaN   \n",
       "4                                                     NaN     NaN   \n",
       "...                                                   ...     ...   \n",
       "186632                                                NaN     NaN   \n",
       "186633                                                NaN     NaN   \n",
       "186634   class=\"a-normal a-align-center a-spacing-smal...     NaN   \n",
       "186635                                                NaN     NaN   \n",
       "186636                                                NaN     NaN   \n",
       "\n",
       "       similar_item tech1  \n",
       "0               NaN   NaN  \n",
       "1               NaN   NaN  \n",
       "2               NaN   NaN  \n",
       "3               NaN   NaN  \n",
       "4               NaN   NaN  \n",
       "...             ...   ...  \n",
       "186632          NaN   NaN  \n",
       "186633          NaN   NaN  \n",
       "186634          NaN   NaN  \n",
       "186635          NaN   NaN  \n",
       "186636          NaN   NaN  \n",
       "\n",
       "[186637 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 10000\n",
    "\n",
    "filename = 'AMAZON_FASHION.json'\n",
    "filepath = data_folderpath.joinpath(filename)\n",
    "\n",
    "iter = pd.read_json(filepath, chunksize = chunksize, lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isFirstChunk = True\n",
    "count = 0\n",
    "\n",
    "for chunk in iter:\n",
    "    #print(count)\n",
    "    if isFirstChunk:\n",
    "        chunk.to_json(\"Sample_Review\", orient='records', lines=True)\n",
    "        review_with_meta = pd.merge(chunk, meta, how=\"left\", on=\"asin\")\n",
    "        review_with_meta[\"reviewText\"] = review_with_meta[\"reviewText\"].replace('\\\"',' ', regex=True).str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "        isFirstChunk = False\n",
    "    else:\n",
    "        chunk.to_json(\"Sample_Review\", orient='records', lines=True)\n",
    "        additional_review_with_meta = pd.merge(chunk, meta, how=\"left\", on=\"asin\")\n",
    "        additional_review_with_meta[\"reviewText\"] = additional_review_with_meta[\"reviewText\"].replace('\\\"',' ', regex=True).str.encode('ascii', 'ignore').str.decode('ascii')\n",
    "        review_with_meta = review_with_meta.append(additional_review_with_meta)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_with_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Loading Sentiment Analysis Model (Self-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vectorizer_full_amazon.pk'\n",
    "filepath = amazon_model_folderpath.joinpath(filename)\n",
    "vectorizer = pk.load(open(filepath, 'rb'))\n",
    "\n",
    "filename = 'logreg_model_full_amazon.sav'\n",
    "filepath = amazon_model_folderpath.joinpath(filename)\n",
    "loaded_model = pk.load(open(filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg_model(TEST_REVIEW, negation_tag):\n",
    "    test_vectors = vectorizer.transform([TEST_REVIEW])\n",
    "    predME = loaded_model.predict(test_vectors)\n",
    "    pred= list(predME)\n",
    "    if pred==[1.0]:\n",
    "        if negation_tag:\n",
    "            prediction = \"Negative\"\n",
    "        else:\n",
    "            prediction = \"Positive\"\n",
    "    else:\n",
    "        if negation_tag:\n",
    "            prediction = \"Positive\"\n",
    "        else:\n",
    "            prediction = \"Negative\"\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logreg_model(\"This shirt is good\", False))\n",
    "print(logreg_model(\"This shirt is bad\", False))\n",
    "print(logreg_model(\"This shirt is not bad\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vectorizer_full_amazon.pk'\n",
    "filepath = amazon_model_folderpath.joinpath(filename)\n",
    "vectorizer = pk.load(open(filepath, 'rb'))\n",
    "\n",
    "filename = 'SVM_model_full_amazon.sav'\n",
    "filepath = amazon_model_folderpath.joinpath(filename)\n",
    "loaded_model = pk.load(open(filepath, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_model(TEST_REVIEW, negation_tag):\n",
    "    test_vectors = vectorizer.transform([TEST_REVIEW])\n",
    "    predME = loaded_model.predict(test_vectors)\n",
    "    pred= list(predME)\n",
    "    if pred==[1.0]:\n",
    "        if negation_tag:\n",
    "            prediction = \"Negative\"\n",
    "        else:\n",
    "            prediction = \"Positive\"\n",
    "    else:\n",
    "        if negation_tag:\n",
    "            prediction = \"Positive\"\n",
    "        else:\n",
    "            prediction = \"Negative\"\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SVM_model(\"This shirt is good\", False))\n",
    "print(SVM_model(\"This shirt is bad\", False))\n",
    "print(SVM_model(\"This shirt is not bad\", False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trained Spacy Model\n",
    "def spacy_model(TEST_REVIEW, negation_tag):\n",
    "\n",
    "    url_start = 'https://danieltanhx.pythonanywhere.com/?input='\n",
    "    url = url_start+str(TEST_REVIEW)\n",
    "    \n",
    "    r = requests.get(url)\n",
    "    pred = json.loads(r.text)[\"Predicted sentiment\"]\n",
    "    \n",
    "    if negation_tag:\n",
    "        if pred == 'Positive':\n",
    "            prediction = 'Negative'\n",
    "        elif pred == 'Negative':\n",
    "            prediction = 'Positive'\n",
    "    else:\n",
    "        prediction = pred\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spacy_model(\"This shirt is good\", False))\n",
    "print(spacy_model(\"This shirt is bad\", False))\n",
    "print(spacy_model(\"This shirt is not bad\", False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Determine Nouns and Adjective Keywords For Different Aspects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mystopwords = stopwords.words(\"english\")\n",
    "# WNlemma = nltk.WordNetLemmatizer()\n",
    "\n",
    "# def pre_process(text):\n",
    "#     text_pos = nltk.pos_tag(nltk.word_tokenize(str(text)))\n",
    "#     tokens   = [ t for t in lemmaNVAR(text_pos) ]\n",
    "#     tokens   = [ t for t in tokens if t not in mystopwords]\n",
    "#     tokens   = [ t for t in tokens if len(t) >= 3 ]\n",
    "#     return(tokens)\n",
    "\n",
    "# def lemmaNVAR(wpos):\n",
    "#     lemmas = []\n",
    "#     for w, pos in wpos:\n",
    "#         if pos == 'NNS':\n",
    "#             lemmas.append(WNlemma.lemmatize(w.lower(), pos = pos[0].lower()))\n",
    "#     return lemmas\n",
    "\n",
    "# reviewText = review_with_meta[\"reviewText\"].apply(pre_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_stopwords =  stopwords.words('english') + ['#']\n",
    "\n",
    "# def preprocess_noun(doc):\n",
    "#     doc = nlp(str(doc))\n",
    "#     noun_list = [token for token in doc if token.pos_== \"NOUN\" and not token.text.isnumeric()]\n",
    "#     noun_list = [noun.lemma_ for noun in noun_list if noun.text not in my_stopwords]\n",
    "\n",
    "#     return noun_list\n",
    "\n",
    "# def preprocess_adjective(doc):\n",
    "#     doc = nlp(str(doc))\n",
    "#     adjective_list = [token for token in doc if token.pos_ == \"ADJ\" and not token.text.isnumeric()]\n",
    "#     adjective_list = [adjective.lemma_ for adjective in adjective_list if adjective.text not in my_stopwords]\n",
    "    \n",
    "#     return adjective_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['review', 'opening', 'hook', 'earring', 'end', 'price']\n",
      "['small', 'expensive', 'high']\n"
     ]
    }
   ],
   "source": [
    "# noun_list = preprocess_noun(review_with_meta[\"reviewText\"].iloc[1])\n",
    "# adjective_list = preprocess_adjective(review_with_meta[\"reviewText\"].iloc[1])\n",
    "\n",
    "# print(noun_list)\n",
    "# print(adjective_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 5065.516253948212\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# noun_list = review_with_meta[\"reviewText\"].apply(preprocess_noun)\n",
    "# end_time = time.time()\n",
    "# print(f\"Time taken {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken 4935.9662482738495\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# start_time = time.time()\n",
    "# adjective_list = review_with_meta[\"reviewText\"].apply(preprocess_adjective)\n",
    "# end_time = time.time()\n",
    "# print(f\"Time taken {end_time-start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('size', 147658),\n",
       " ('quality', 84732),\n",
       " ('color', 69443),\n",
       " ('dress', 68854),\n",
       " ('shirt', 62614),\n",
       " ('price', 59233),\n",
       " ('material', 58669),\n",
       " ('product', 57751),\n",
       " ('time', 53015),\n",
       " ('picture', 40356),\n",
       " ('bit', 38459),\n",
       " ('top', 37026),\n",
       " ('fit', 34854),\n",
       " ('one', 33885),\n",
       " ('day', 32619),\n",
       " ('review', 32392),\n",
       " ('ring', 29979),\n",
       " ('way', 29466),\n",
       " ('shoe', 29085),\n",
       " ('year', 28858),\n",
       " ('daughter', 28344),\n",
       " ('fabric', 28222),\n",
       " ('hat', 28190),\n",
       " ('pair', 26899),\n",
       " ('bag', 26738),\n",
       " ('lot', 25455),\n",
       " ('item', 25403),\n",
       " ('gift', 23940),\n",
       " ('thing', 22321),\n",
       " ('month', 21587),\n",
       " ('medium', 21358),\n",
       " ('pant', 20744),\n",
       " ('belt', 18589),\n",
       " ('money', 17995),\n",
       " ('pocket', 17696),\n",
       " ('compliment', 17355),\n",
       " ('bottom', 17296),\n",
       " ('side', 17235),\n",
       " ('son', 16899),\n",
       " ('waist', 16800),\n",
       " ('star', 16697),\n",
       " ('foot', 16277),\n",
       " ('wallet', 16069),\n",
       " ('design', 15851),\n",
       " ('style', 15704),\n",
       " ('length', 15430),\n",
       " ('purchase', 15363),\n",
       " ('week', 15323),\n",
       " ('piece', 15192),\n",
       " ('suit', 14916),\n",
       " ('strap', 14601),\n",
       " ('problem', 13849),\n",
       " ('necklace', 13316),\n",
       " ('glass', 13238),\n",
       " ('head', 13171),\n",
       " ('look', 13127),\n",
       " ('costume', 13105),\n",
       " ('back', 13053),\n",
       " ('purse', 13049),\n",
       " ('band', 13003),\n",
       " ('sock', 12884),\n",
       " ('watch', 12877),\n",
       " ('husband', 12681),\n",
       " ('shipping', 12420),\n",
       " ('weight', 12254),\n",
       " ('part', 12217),\n",
       " ('bracelet', 11833),\n",
       " ('skirt', 11803),\n",
       " ('leather', 11734),\n",
       " ('sleeve', 11268),\n",
       " ('love', 11239),\n",
       " ('card', 11223),\n",
       " ('seller', 11215),\n",
       " ('jacket', 11123),\n",
       " ('friend', 10946),\n",
       " ('baby', 10529),\n",
       " ('photo', 10514),\n",
       " ('earring', 10494),\n",
       " ('case', 10469),\n",
       " ('work', 10278),\n",
       " ('arm', 10265),\n",
       " ('zipper', 10198),\n",
       " ('chain', 10031),\n",
       " ('girl', 9941),\n",
       " ('boot', 9939),\n",
       " ('area', 9815),\n",
       " ('bra', 9757),\n",
       " ('woman', 9731),\n",
       " ('issue', 9431),\n",
       " ('wife', 9222),\n",
       " ('hand', 9194),\n",
       " ('leg', 9148),\n",
       " ('brand', 9077),\n",
       " ('person', 8987),\n",
       " ('shoulder', 8893),\n",
       " ('summer', 8740),\n",
       " ('ear', 8735),\n",
       " ('order', 8677),\n",
       " ('place', 8633),\n",
       " ('outfit', 8532)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# noun_toks = [toks for tokens in noun_list for toks in tokens]\n",
    "# fdist = FreqDist(noun_toks)\n",
    "# fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 153509),\n",
       " ('small', 131651),\n",
       " ('good', 112526),\n",
       " ('nice', 96258),\n",
       " ('little', 71422),\n",
       " ('large', 69101),\n",
       " ('perfect', 67577),\n",
       " ('cute', 63769),\n",
       " ('big', 61944),\n",
       " ('comfortable', 50356),\n",
       " ('beautiful', 40741),\n",
       " ('cheap', 34467),\n",
       " ('fit', 32519),\n",
       " ('soft', 29777),\n",
       " ('long', 28413),\n",
       " ('short', 26075),\n",
       " ('tight', 25992),\n",
       " ('happy', 25985),\n",
       " ('old', 25805),\n",
       " ('thin', 23655),\n",
       " ('different', 19120),\n",
       " ('well', 18450),\n",
       " ('first', 16898),\n",
       " ('many', 16612),\n",
       " ('warm', 16389),\n",
       " ('easy', 16097),\n",
       " ('sure', 15922),\n",
       " ('high', 15475),\n",
       " ('worth', 15422),\n",
       " ('bad', 15386),\n",
       " ('pretty', 15037),\n",
       " ('black', 14962),\n",
       " ('light', 14061),\n",
       " ('right', 13950),\n",
       " ('loose', 13863),\n",
       " ('true', 13239),\n",
       " ('disappointed', 13079),\n",
       " ('excellent', 12646),\n",
       " ('thick', 12443),\n",
       " ('new', 11834),\n",
       " ('fine', 11758),\n",
       " ('much', 11705),\n",
       " ('awesome', 11179),\n",
       " ('white', 11047),\n",
       " ('able', 10926),\n",
       " ('comfy', 10532),\n",
       " ('extra', 10447),\n",
       " ('heavy', 10443),\n",
       " ('amazing', 10356),\n",
       " ('hard', 10091),\n",
       " ('pleased', 10024),\n",
       " ('cool', 9154),\n",
       " ('adorable', 9098),\n",
       " ('low', 9013),\n",
       " ('honest', 8680),\n",
       " ('several', 8287),\n",
       " ('real', 8147),\n",
       " ('wide', 8147),\n",
       " ('fast', 7971),\n",
       " ('sturdy', 7780),\n",
       " ('top', 7388),\n",
       " ('second', 7157),\n",
       " ('tall', 7086),\n",
       " ('huge', 7078),\n",
       " ('blue', 6965),\n",
       " ('ok', 6744),\n",
       " ('normal', 6686),\n",
       " ('bright', 6677),\n",
       " ('wrong', 6648),\n",
       " ('sexy', 6648),\n",
       " ('poor', 6633),\n",
       " ('expensive', 6631),\n",
       " ('dark', 6531),\n",
       " ('favorite', 6526),\n",
       " ('durable', 6511),\n",
       " ('lovely', 6020),\n",
       " ('gorgeous', 5994),\n",
       " ('next', 5940),\n",
       " ('tiny', 5884),\n",
       " ('full', 5849),\n",
       " ('glad', 5711),\n",
       " ('last', 5639),\n",
       " ('wonderful', 5520),\n",
       " ('red', 5405),\n",
       " ('regular', 5372),\n",
       " ('hot', 5371),\n",
       " ('enough', 5259),\n",
       " ('green', 5205),\n",
       " ('super', 5146),\n",
       " ('lightweight', 5125),\n",
       " ('satisfied', 5051),\n",
       " ('medium', 5024),\n",
       " ('cold', 5009),\n",
       " ('flattering', 4980),\n",
       " ('weird', 4891),\n",
       " ('free', 4875),\n",
       " ('stylish', 4857),\n",
       " ('uncomfortable', 4728),\n",
       " ('less', 4613),\n",
       " ('quick', 4582)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjective_toks = [toks for tokens in adjective_list for toks in tokens]\n",
    "# fdist = FreqDist(adjective_toks)\n",
    "# fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Aspect Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_list = ['size', 'comfort', 'appearance', 'quality', 'price', 'delivery']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Types from common Amazon data\n",
    "noun_keywords = { 'size'          : ['size', 'fit', 'length'],\n",
    "                  'comfort'       : [],\n",
    "                  'appearance'    : ['colour', 'picture', 'design', 'style', 'photo'],\n",
    "                  'quality'       : ['quality', 'material', 'fabric', 'leather'],\n",
    "                  'price'         : ['price', 'money'],\n",
    "                  'delivery'      : ['time', 'day', 'seller', 'shipping', 'order']}\n",
    "                     \n",
    "for topic in topic_list:\n",
    "    filename = topic+'.csv'\n",
    "    filepath = wordnet_folderpath.joinpath(topic).joinpath(filename)\n",
    "    \n",
    "    with open(filepath, newline='\\n') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    nouns = noun_keywords[topic] + [pair[0] for pair in data if pair[1] == 'noun']\n",
    "    noun_keywords[topic] = list(dict.fromkeys(nouns))\n",
    "    print(noun_keywords[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect Types from common Amazon data\n",
    "adjective_keywords = { 'size'          : ['small', 'large', 'little', 'big', 'fit', 'long', 'short', 'tight', 'loose', 'medium', 'tiny', 'huge'],\n",
    "                       'comfort'       : ['comfortable', 'uncomfortable', 'soft', 'lightweight','comfy'],\n",
    "                       'appearance'    : ['beautiful', 'stylish', 'flattering', 'gorgeoous', 'lovely', 'sexy', 'adorable', 'cool'],\n",
    "                       'quality'       : ['durable', 'sturdy', 'heavy', 'thick', 'new', 'old', 'hard'],\n",
    "                       'price'         : ['cheap', 'expensive', 'honest', 'worth'],\n",
    "                       'delivery'      : ['fast', 'quick']}\n",
    "\n",
    "for topic in topic_list:\n",
    "    filename = topic+'.csv'\n",
    "    filepath = wordnet_folderpath.joinpath(topic).joinpath(filename)\n",
    "    \n",
    "    with open(filepath, newline='\\n') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    adjectives = adjective_keywords[topic] + [pair[0] for pair in data if pair[1] == 'adj']\n",
    "    adjective_keywords[topic] = list(dict.fromkeys(adjectives))\n",
    "    print(adjective_keywords[topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_noun_head(token):\n",
    "    #print(f\"Finding Token: {token}\")\n",
    "    #print(f\"Token Head: {token.head}\")\n",
    "    if token.head.pos_ == \"NOUN\" or token.head.pos_ == \"NOUN\" or token.head.dep_ == \"ROOT\":\n",
    "        #print(\"Found\")\n",
    "        return token.head\n",
    "    else:\n",
    "        #print(\"Continue Search\")\n",
    "        return find_noun_head(token.head)\n",
    "    \n",
    "def find_negation_tag(adjective):\n",
    "    #print(f\"Finding Negatation for Adjective: {adjective}\")\n",
    "    #print(f\"Token Head: {adjective.head}\")\n",
    "    if adjective.head.dep_ == \"neg\":\n",
    "        return True\n",
    "    elif adjective.head.pos_ == \"AUX\":\n",
    "        for child in adjective.head.children:\n",
    "            if child.dep_ =='neg':\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def find_span_start(sent, token):\n",
    "    #for tok in sent:\n",
    "    #    print(f\"{tok.i}: {tok} - {tok.pos_}\")\n",
    "    #print(f\"Start {token.i}: {token} - {token.pos_}\")  \n",
    "    if token.i == sent.start or sent[token.i-1].pos_ == \"PUNCT\":\n",
    "        #print(token.i)\n",
    "        return token.i\n",
    "    else:\n",
    "        #print(token.i-1)\n",
    "        return find_span_start(sent, sent[token.i-1])\n",
    "\n",
    "def find_span_end(sent, token):\n",
    "    #for tok in sent:\n",
    "    #    print(f\"{tok.i}: {tok} - {tok.pos_}\")\n",
    "    #print(f\"End {token.i}: {token} - {token.pos_}\")        \n",
    "    if token.i+1 == sent.end or sent[token.i+1].pos_ == \"PUNCT\":\n",
    "        #print(token.i+1)\n",
    "        return token.i+1\n",
    "    else:\n",
    "        #print(token.i+1)\n",
    "        return find_span_end(sent, sent[token.i+1])\n",
    "    \n",
    "def match_topics(noun_token, adjective_token, topic_list, noun_keywords, adjective_keywords):\n",
    "\n",
    "    for topic in topic_list:\n",
    "        if noun_token.lemma_ in noun_keywords[topic]:\n",
    "            return topic\n",
    "        elif adjective_token.lemma_ in adjective_keywords[topic]:\n",
    "            return topic\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adsa_model(review, DEBUG = False, SENTI_MODEL = 'logreg'):\n",
    "\n",
    "    #Model Setting\n",
    "    SENTI_MODEL = SENTI_MODEL\n",
    "    DEBUG = DEBUG\n",
    "\n",
    "    #Model    \n",
    "    doc = nlp(review)\n",
    "\n",
    "    topic_prediction = {'size'          : None,\n",
    "                        'comfort'       : None,\n",
    "                        'appearance'    : None,\n",
    "                        'quality'       : None,\n",
    "                        'price'         : None, \n",
    "                        'delivery'      : None}\n",
    "\n",
    "    sent_count = 0\n",
    "    for sent in doc.sents:\n",
    "        sent_count += 1\n",
    "\n",
    "        descriptor_pair = []\n",
    "        adjectives = [tok for tok in sent if tok.pos_ == \"ADJ\"]\n",
    "        pronouns = [tok for tok in sent if tok.pos_ == \"PRON\"]\n",
    "        nouns = [tok for tok in sent if tok.pos_ == \"NOUN\"]\n",
    "        negations = [tok for tok in sent if tok.dep_ == \"neg\"]\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(f\"Sentence {sent_count}: {sent}\")\n",
    "            displacy.render(sent, style=\"dep\")\n",
    "            print(f\"Adjectives: {adjectives}\")\n",
    "            print(f\"Nouns: {nouns}\")\n",
    "            print(f\"Pronouns: {pronouns}\")\n",
    "            print(f\"Negations: {negations}\")\n",
    "\n",
    "        for adjective in adjectives:\n",
    "            isFound = False\n",
    "            topic = None\n",
    "            \n",
    "            try:\n",
    "                descriptor = \"\"\n",
    "                for child in adjective.children:\n",
    "                    if child.pos_ == \"ADV\":\n",
    "                        descriptor += child.text + \" \"\n",
    "                descriptor += adjective.text\n",
    "\n",
    "                negation_tag = find_negation_tag(adjective)\n",
    "\n",
    "\n",
    "                #Direct (i.e. The dress has a beautiful colour)\n",
    "                noun = find_noun_head(adjective)\n",
    "                if noun.pos_ == \"NOUN\" or noun.pos_ == \"PRON\":\n",
    "                    trace = f\"Direct Reference Detected for Adjective: {adjective}\"\n",
    "                    for chunk in sent.noun_chunks:\n",
    "                        if chunk.root == noun and adjective not in chunk:\n",
    "                            isFound = True\n",
    "                            noun_subj = chunk\n",
    "                        else:\n",
    "                            isFound = True\n",
    "                            noun_subj = noun             \n",
    "\n",
    "                #Passive Voice (i.e. Colour of the dress was beautiful)\n",
    "                elif adjective.head.pos_ == \"AUX\" or adjective.head.pos_ == \"VERB\":\n",
    "                    trace = f\"Indirect Reference Detected for Adjective: {adjective}\"\n",
    "                    for child in adjective.head.children:\n",
    "                        if child.pos_ == \"NOUN\" or child.pos_ == \"PRON\":\n",
    "                            noun = child                      \n",
    "                            for chunk in sent.noun_chunks:\n",
    "                                if chunk.root == child and adjective not in chunk:\n",
    "                                    isFound = True\n",
    "                                    noun_subj = chunk\n",
    "                                else:\n",
    "                                    isFound = True\n",
    "                                    noun_subj = noun\n",
    "                                    \n",
    "                #Guessing when improper grammer is used (i.e. Colour of the dress was beautiful vs also dress colour beautiful)\n",
    "                else:\n",
    "                    trace = f\"Guessing for Adjective: {adjective}\\n\"\n",
    "                    start = find_span_start(sent, adjective)\n",
    "                    end = find_span_end(sent, adjective)\n",
    "                    extract = sent[start:end]\n",
    "                    trace += f\"Extract: {extract}\"\n",
    "\n",
    "                    for token in extract:\n",
    "                        if token.pos_ == \"NOUN\" and token.dep_ == \"nsubj\":\n",
    "                            isFound = True\n",
    "                            noun_subj = token\n",
    "                        if token.dep_ == \"neg\":\n",
    "                            negation_tag = True\n",
    "            except:\n",
    "                isFound = False\n",
    "\n",
    "            if isFound:\n",
    "                topic = match_topics(noun, adjective, topic_list, noun_keywords, adjective_keywords)\n",
    "            else:\n",
    "                noun_subj = None\n",
    "                topic = match_topics(nlp(\"I\")[0], adjective, topic_list, noun_keywords, adjective_keywords)\n",
    "\n",
    "            if topic != None:\n",
    "                if noun_subj == None:                    \n",
    "                    if SENTI_MODEL == 'logreg':\n",
    "                        prediction = logreg_model(str(descriptor) + \" \" + str(topic), negation_tag)\n",
    "                    elif SENTI_MODEL == 'svm':\n",
    "                        prediction = SVM_model(str(descriptor) + \" \" + str(topic), negation_tag)\n",
    "                    elif SENTI_MODEL == 'spacy':\n",
    "                        prediction = spacy_model(str(descriptor) + \" \" + str(topic), negation_tag)\n",
    "                else:\n",
    "                    if SENTI_MODEL == 'logreg':\n",
    "                        prediction = logreg_model(str(descriptor) + \" \" + str(noun_subj), negation_tag)\n",
    "                    elif SENTI_MODEL == 'svm':\n",
    "                        prediction = SVM_model(str(descriptor) + \" \" + str(noun_subj), negation_tag)\n",
    "                    elif SENTI_MODEL == 'spacy':\n",
    "                        prediction = spacy_model(str(descriptor) + \" \" + str(noun_subj), negation_tag)\n",
    "                    \n",
    "\n",
    "                descriptor_pair.append((descriptor, noun_subj, negation_tag, topic, prediction))\n",
    "                topic_prediction[topic] = prediction\n",
    "\n",
    "            if DEBUG:\n",
    "                print(f\"Descriptor_pair: {descriptor_pair}\")\n",
    "                print(\"\")\n",
    "\n",
    "    print(f\"Review: {review}\")\n",
    "    print(f\"Topic_prediction: {topic_prediction}\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(\"\")\n",
    "    \n",
    "    return topic_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5a: Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_evaluation(spans):\n",
    "    \n",
    "    topic_list = ['size', 'comfort', 'appearance', 'quality', 'price', 'delivery']    \n",
    "    topic_prediction = {'size'          : None,\n",
    "                        'comfort'       : None,\n",
    "                        'appearance'    : None,\n",
    "                        'quality'       : None,\n",
    "                        'price'         : None, \n",
    "                        'delivery'      : None}\n",
    "    \n",
    "    if not isinstance(spans, float):\n",
    "        for span in spans:\n",
    "            topic_prediction[span['label'].lower()] = \"Found\"\n",
    "\n",
    "    return topic_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(series):\n",
    "    topic_list = ['size', 'comfort', 'appearance', 'quality', 'price', 'delivery']    \n",
    "    \n",
    "    correct = 0\n",
    "    for topic in topic_list:\n",
    "        if series['topics_target'][topic] != None and series['topics_predicted'][topic] != None:\n",
    "            correct += 1\n",
    "        elif series['topics_target'][topic] == None and series['topics_predicted'][topic] == None:\n",
    "            correct += 1\n",
    "    \n",
    "    accuracy = correct / len(topic_list)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(dict):\n",
    "    topic_list = ['size', 'comfort', 'appearance', 'quality', 'price', 'delivery']\n",
    "    \n",
    "    list = []\n",
    "    \n",
    "    for topic in topic_list:\n",
    "        if dict[topic] != None:\n",
    "            list.append(1)\n",
    "        else:\n",
    "            list.append(0)\n",
    "    \n",
    "    return list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5b: Apply Model to Evaluation Dataset (Amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'amazon_adsa_ner.jsonl'\n",
    "filepath = data_folderpath.joinpath(filename)\n",
    "amazon_test_data = pd.read_json(filepath, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test_data['topics_target'] = amazon_test_data['spans'].apply(process_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test_data['topics_predicted'] = amazon_test_data['text'].apply(adsa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test_data['accuracy'] = amazon_test_data[['topics_target', 'topics_predicted']].apply(accuracy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test_data['target_list'] = amazon_test_data['topics_target'].apply(to_list)\n",
    "amazon_test_data['predicted_list'] = amazon_test_data['topics_predicted'].apply(to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_target_list = []\n",
    "amazon_predicted_list = []\n",
    "\n",
    "for index in range(len(amazon_test_data)):\n",
    "    amazon_target_list.append(amazon_test_data.iloc[index]['target_list'])\n",
    "    amazon_predicted_list.append(amazon_test_data.iloc[index]['predicted_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(multilabel_confusion_matrix(amazon_target_list, amazon_predicted_list))\n",
    "print(classification_report(amazon_target_list, amazon_predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_test_data[amazon_test_data['accuracy'] != 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 7\n",
    "\n",
    "print(amazon_test_data.iloc[index]['text'])\n",
    "print(amazon_test_data.iloc[index]['topics_target'])\n",
    "print(amazon_test_data.iloc[index]['topics_predicted'])\n",
    "print(amazon_test_data.iloc[index]['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.average(amazon_test_data['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Apply Model to Evaluation Dataset (Shopee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'shopee_adsa_ner.jsonl'\n",
    "filepath = data_folderpath.joinpath(filename)\n",
    "shopee_test_data = pd.read_json(filepath, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopee_test_data['topics_target'] = shopee_test_data['spans'].apply(process_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopee_test_data['topics_predicted'] = shopee_test_data['text'].apply(adsa_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopee_test_data['accuracy'] = shopee_test_data[['topics_target', 'topics_predicted']].apply(accuracy, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopee_test_data['target_list'] = shopee_test_data['topics_target'].apply(to_list)\n",
    "shopee_test_data['predicted_list'] = shopee_test_data['topics_predicted'].apply(to_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shopee_target_list = []\n",
    "shopee_predicted_list = []\n",
    "\n",
    "for index in range(len(shopee_test_data)):\n",
    "    shopee_target_list.append(shopee_test_data.iloc[index]['target_list'])\n",
    "    shopee_predicted_list.append(shopee_test_data.iloc[index]['predicted_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(multilabel_confusion_matrix(shopee_target_list, shopee_predicted_list))\n",
    "print(classification_report(shopee_target_list, shopee_predicted_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(shopee_test_data['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Apply Model to Selected Brands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_name = \"adidas\"\n",
    "\n",
    "print(meta[meta[\"brand\"]==brand_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_review_with_meta = review_with_meta[review_with_meta[\"brand\"]==brand_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_review_with_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(reduced_review_with_meta)):\n",
    "    (brand, product, review, rating) = (str(reduced_review_with_meta.iloc[index][\"brand\"]), \n",
    "                                        str(reduced_review_with_meta.iloc[index][\"title\"]), \n",
    "                                        str(reduced_review_with_meta.iloc[index][\"reviewText\"]),\n",
    "                                        str(reduced_review_with_meta.iloc[index][\"overall\"]))\n",
    "\n",
    "    print(f\"Index: {index}\")\n",
    "    print(f\"Brand: {brand}\")\n",
    "    print(f\"Product: {product}\")\n",
    "    print(f\"Review Text: {review}\")\n",
    "    print(f\"Product Rating: {rating}\")\n",
    "    adsa_model(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
